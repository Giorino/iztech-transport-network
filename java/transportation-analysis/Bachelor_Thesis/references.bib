
@book{hawkins_identification_1980,
	location = {Dordrecht},
	title = {Identification of Outliers},
	rights = {http://www.springer.com/tdm},
	isbn = {978-94-015-3996-8 978-94-015-3994-4},
	url = {http://link.springer.com/10.1007/978-94-015-3994-4},
	publisher = {Springer Netherlands},
	author = {Hawkins, D. M.},
	urldate = {2024-06-23},
	date = {1980},
	langid = {english},
	doi = {10.1007/978-94-015-3994-4},
}

@article{boukerche_outlier_2020,
	title = {Outlier Detection: Methods, Models, and Classification},
	volume = {53},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3381028},
	doi = {10.1145/3381028},
	shorttitle = {Outlier Detection},
	abstract = {Over the past decade, we have witnessed an enormous amount of research effort dedicated to the design of efficient outlier detection techniques while taking into consideration efficiency, accuracy, high-dimensional data, and distributed environments, among other factors. In this article, we present and examine these characteristics, current solutions, as well as open challenges and future research directions in identifying new outlier detection strategies. We propose a taxonomy of the recently designed outlier detection strategies while underlying their fundamental characteristics and properties. We also introduce several newly trending outlier detection methods designed for high-dimensional data, data streams, big data, and minimally labeled data. Last, we review their advantages and limitations and then discuss future and new challenging issues.},
	pages = {55:1--55:37},
	number = {3},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Boukerche, Azzedine and Zheng, Lining and Alfandi, Omar},
	urldate = {2024-06-23},
	date = {2020-06-12},
	keywords = {Outlier detection, anomaly detection, semi-supervised learning, unsupervised learning},
}

@inproceedings{chen_xgboost_2016,
	location = {San Francisco California {USA}},
	title = {{XGBoost}: A Scalable Tree Boosting System},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	shorttitle = {{XGBoost}},
	eventtitle = {{KDD} '16: The 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {785--794},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Chen, Tianqi and Guestrin, Carlos},
	urldate = {2024-06-20},
	date = {2016-08-13},
	langid = {english},
}

@article{wu_prediction_2010,
	title = {Prediction Modeling Using {EHR} Data: Challenges, Strategies, and a Comparison of Machine Learning Approaches},
	volume = {48},
	issn = {0025-7079},
	url = {https://journals.lww.com/00005650-201006001-00017},
	doi = {10.1097/MLR.0b013e3181de9e17},
	shorttitle = {Prediction Modeling Using {EHR} Data},
	pages = {S106--S113},
	number = {6},
	journaltitle = {Medical Care},
	author = {Wu, Jionglin and Roy, Jason and Stewart, Walter F.},
	urldate = {2024-06-20},
	date = {2010-06},
	langid = {english},
}

@article{wu_prediction_2010-1,
	title = {Prediction Modeling Using {EHR} Data: Challenges, Strategies, and a Comparison of Machine Learning Approaches},
	volume = {48},
	issn = {0025-7079},
	url = {https://journals.lww.com/00005650-201006001-00017},
	doi = {10.1097/MLR.0b013e3181de9e17},
	shorttitle = {Prediction Modeling Using {EHR} Data},
	pages = {S106--S113},
	number = {6},
	journaltitle = {Medical Care},
	author = {Wu, Jionglin and Roy, Jason and Stewart, Walter F.},
	urldate = {2024-06-20},
	date = {2010-06},
	langid = {english},
}

@article{noauthor_notitle_nodate,
}

@article{qayyum_secure_2021,
	title = {Secure and Robust Machine Learning for Healthcare: A Survey},
	volume = {14},
	issn = {1941-1189},
	url = {https://ieeexplore.ieee.org/abstract/document/9153891},
	doi = {10.1109/RBME.2020.3013489},
	shorttitle = {Secure and Robust Machine Learning for Healthcare},
	abstract = {Recent years have witnessed widespread adoption of machine learning ({ML})/deep learning ({DL}) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from one-dimensional heart signals to computer-aided diagnosis ({CADx}) using multi-dimensional medical images. Notwithstanding the impressive performance of {ML}/{DL}, there are still lingering doubts regarding the robustness of {ML}/{DL} in healthcare settings (which is traditionally considered quite challenging due to the myriad security and privacy issues involved), especially in light of recent results that have shown that {ML}/{DL} are vulnerable to adversarial attacks. In this paper, we present an overview of various application areas in healthcare that leverage such techniques from security and privacy point of view and present associated challenges. In addition, we present potential methods to ensure secure and privacy-preserving {ML} for healthcare applications. Finally, we provide insight into the current research challenges and promising directions for future research.},
	pages = {156--180},
	journaltitle = {{IEEE} Reviews in Biomedical Engineering},
	author = {Qayyum, Adnan and Qadir, Junaid and Bilal, Muhammad and Al-Fuqaha, Ala},
	urldate = {2024-06-19},
	date = {2021},
	note = {Conference Name: {IEEE} Reviews in Biomedical Engineering},
	keywords = {Adversarial {ML}, Adversarial machine learning, Machine learning, Medical services, Privacy, Robustness, healthcare, privacy preserving {ML}, robust {ML}, secure {ML}},
}

@article{wu_prediction_2010-2,
	title = {Prediction Modeling Using {EHR} Data: Challenges, Strategies, and a Comparison of Machine Learning Approaches},
	volume = {48},
	issn = {0025-7079},
	url = {https://journals.lww.com/lww-medicalcare/fulltext/2010/06001/Prediction_Modeling_Using_EHR_Data__Challenges,.17.aspx?casa_token=clah2vUNTBoAAAAA:1RnAgh98q7kiZervLDZ7u1Q0Q9PJbAi4FcINymqJmzWDwAHoOBCn-ZFy9METhxMX_3onxnjMz1yQ0yX_Cr5liya9sqMt},
	doi = {10.1097/MLR.0b013e3181de9e17},
	shorttitle = {Prediction Modeling Using {EHR} Data},
	abstract = {Background: 
          Electronic health record ({EHR}) databases contain vast amounts of information about patients. Machine learning techniques such as Boosting and support vector machine ({SVM}) can potentially identify patients at high risk for serious conditions, such as heart disease, from {EHR} data. However, these techniques have not yet been widely tested.
          Objective: 
          To model detection of heart failure more than 6 months before the actual date of clinical diagnosis using machine learning techniques applied to {EHR} data. To compare the performance of logistic regression, {SVM}, and Boosting, along with various variable selection methods in heart failure prediction.
          Research Design: 
          Geisinger Clinic primary care patients with data in the {EHR} data from 2001 to 2006 diagnosed with heart failure between 2003 and 2006 were identified. Controls were randomly selected matched on sex, age, and clinic for this nested case-control study.
          Measures: 
          Area under the curve ({AUC}) of receiver operator characteristic curve was computed for each method using 10-fold cross-validation. The number of variables selected by each method was compared.
          Results: 
          Logistic regression with model selection based on Bayesian information criterion provided the most parsimonious model, with about 10 variables selected on average, while maintaining a high {AUC} (0.77 in 10-fold cross-validation). Boosting with strict variable importance threshold provided similar performance.
          Conclusions: 
          Heart failure was predicted more than 6 months before clinical diagnosis, with {AUC} of about 0.76, using logistic regression and Boosting. These results were achieved even with strict model selection criteria. {SVM} had the poorest performance, possibly because of imbalanced data.},
	pages = {S106},
	number = {6},
	journaltitle = {Medical Care},
	author = {Wu, Jionglin and Roy, Jason and Stewart, Walter F.},
	urldate = {2024-06-19},
	date = {2010-06},
	langid = {american},
}

@article{breiman_random_2001,
	title = {Random Forests},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	pages = {5--32},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Breiman, Leo},
	urldate = {2024-06-18},
	date = {2001-10-01},
	langid = {english},
	keywords = {classification, ensemble, regression},
}

@article{suykens_least_1999,
	title = {Least Squares Support Vector Machine Classifiers},
	volume = {9},
	issn = {1573-773X},
	url = {https://doi.org/10.1023/A:1018628609742},
	doi = {10.1023/A:1018628609742},
	abstract = {In this letter we discuss a least squares version for support vector machine ({SVM}) classifiers. Due to equality type constraints in the formulation, the solution follows from solving a set of linear equations, instead of quadratic programming for classical {SVM}'s. The approach is illustrated on a two-spiral benchmark classification problem.},
	pages = {293--300},
	number = {3},
	journaltitle = {Neural Processing Letters},
	shortjournal = {Neural Processing Letters},
	author = {Suykens, J.A.K. and Vandewalle, J.},
	urldate = {2024-06-18},
	date = {1999-06-01},
	langid = {english},
	keywords = {classification, linear least squares, radial basis function kernel, support vector machines},
}

@article{cover_nearest_1967,
	title = {Nearest neighbor pattern classification},
	volume = {13},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/1053964?casa_token=QI2wkihvArIAAAAA:e6kHL3AmaROEuURQpXSeprymGT361BX3oGlsEWagmMi9QFZBKFohX81Ijklg739Um1PMAQjZClB8bQ},
	doi = {10.1109/TIT.1967.1053964},
	abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of {errorRof} such a rule must be at least as great as the Bayes probability of {errorR}{\textasciicircum}{\textbackslash}ast–the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in {theM}-category case {thatR}{\textasciicircum}{\textbackslash}ast łeq R łeq R{\textasciicircum}{\textbackslash}ast(2 –{MR}{\textasciicircum}{\textbackslash}ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
	pages = {21--27},
	number = {1},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Cover, T. and Hart, P.},
	urldate = {2024-06-18},
	date = {1967-01},
	note = {Conference Name: {IEEE} Transactions on Information Theory},
}

@online{noauthor_bachelor_nodate,
	title = {Bachelor Thesis Marc Fuhrer},
	url = {https://www.overleaf.com/project/66684c09fbc4261bef7bdc94},
	abstract = {An online {LaTeX} editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of {LaTeX} templates, and more.},
	urldate = {2024-06-18},
	langid = {english},
}

@article{hariri_uncertainty_2019,
	title = {Uncertainty in big data analytics: survey, opportunities, and challenges},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0206-3},
	doi = {10.1186/s40537-019-0206-3},
	shorttitle = {Uncertainty in big data analytics},
	abstract = {Big data analytics has gained wide attention from both academia and industry as the demand for understanding trends in massive datasets increases. Recent developments in sensor networks, cyber-physical systems, and the ubiquity of the Internet of Things ({IoT}) have increased the collection of data (including health care, social media, smart cities, agriculture, finance, education, and more) to an enormous scale. However, the data collected from sensors, social media, financial records, etc. is inherently uncertain due to noise, incompleteness, and inconsistency. The analysis of such massive amounts of data requires advanced analytical techniques for efficiently reviewing and/or predicting future courses of action with high precision and advanced decision-making strategies. As the amount, variety, and speed of data increases, so too does the uncertainty inherent within, leading to a lack of confidence in the resulting analytics process and decisions made thereof. In comparison to traditional data techniques and platforms, artificial intelligence techniques (including machine learning, natural language processing, and computational intelligence) provide more accurate, faster, and scalable results in big data analytics. Previous research and surveys conducted on big data analytics tend to focus on one or two techniques or specific application domains. However, little work has been done in the field of uncertainty when applied to big data analytics as well as in the artificial intelligence techniques applied to the datasets. This article reviews previous work in big data analytics and presents a discussion of open challenges and future directions for recognizing and mitigating uncertainty in this domain.},
	pages = {44},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {J Big Data},
	author = {Hariri, Reihaneh H. and Fredericks, Erik M. and Bowers, Kate M.},
	urldate = {2024-06-18},
	date = {2019-06-04},
	langid = {english},
	keywords = {Artificial intelligence, Big data, Big data analytics, Uncertainty},
}

@article{garg_role_2021,
	title = {Role of machine learning in medical research: A survey},
	volume = {40},
	issn = {1574-0137},
	url = {https://www.sciencedirect.com/science/article/pii/S1574013721000101},
	doi = {10.1016/j.cosrev.2021.100370},
	shorttitle = {Role of machine learning in medical research},
	abstract = {Machine learning is one of the essential and effective tools in analyzing highly complex medical data. With vast amounts of medical data being generated, there is an urgent need to effectively use this data to benefit the medical and health care sectors all across the world. This survey paper presents a systematic literature review for the investigation of various machine learning techniques used for numerous medical applications which are published in highly reputable venues in recent years. Considering only the recent work, we are able to survey the current machine learning and deep learning models that are being used for medical data. This literature review identifies a clear shift of artificial intelligence techniques used in the medical domain, with deep learning methods taking precedence over machine learning methods.},
	pages = {100370},
	journaltitle = {Computer Science Review},
	shortjournal = {Computer Science Review},
	author = {Garg, Arunim and Mago, Vijay},
	urldate = {2024-06-18},
	date = {2021-05-01},
	keywords = {Deep learning, Machine learning, Medical data, Medical research},
}

@article{rajkomar_alvin_machine_2019,
	title = {Machine Learning in Medicine},
	volume = {380},
	url = {https://www.nejm.org/doi/full/10.1056/NEJMra1814259},
	doi = {10.1056/NEJMra1814259},
	abstract = {In this view of the future of medicine, patient–provider interactions are informed and supported by massive amounts of data from interactions with similar patients. These data are collected and curated to provide the latest evidence-based assessment and recommendations.},
	pages = {1347--1358},
	number = {14},
	journaltitle = {New England Journal of Medicine},
	author = {{Rajkomar Alvin} and {Dean Jeffrey} and {Kohane Isaac}},
	urldate = {2024-06-16},
	date = {2019-04-04},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://www.nejm.org/doi/pdf/10.1056/{NEJMra}1814259},
}

@article{jovel_introduction_2021,
	title = {An Introduction to Machine Learning Approaches for Biomedical Research},
	volume = {8},
	issn = {2296-858X},
	url = {https://www.frontiersin.org/articles/10.3389/fmed.2021.771607},
	doi = {10.3389/fmed.2021.771607},
	abstract = {Machine learning ({ML}) approaches are a collection of algorithms that attempt to extract patterns from data and associate such patterns with discrete classes of samples in the data – e.g. given a series of features describing persons, a {ML} model predicts whether a person is diseased or healthy, or given features of animals, it predicts weather an animal is treated or control, or whether molecules have the potential to interact or not, etc. {ML} approaches can also find such patterns in an agnostic manner, i.e. without having information about the classes. Respectively, those methods are referred to as supervised and unsupervised {ML}. A third type of {ML} is reinforcement learning, which attempts to find a sequence of actions to optimize a utility function, but given its complexity it will not be discussed here. All of these methods are becoming increasingly popular in biomedical research in quite diverse areas including drug design, stratification of patients, medical images analysis, molecular interactions, prediction of therapy outcomes and many more. We describe several {ML} techniques, pertaining to the first two families mentioned above by illustrating a series of prototypical examples using state-of-the-art computational approaches. We focus on concepts rather than procedures, as our goal is to attract the attention of researchers in biomedicine towards the plethora of powerful {ML} methods and their potential to leverage basic and applied research programs.},
	journaltitle = {Frontiers in Medicine},
	shortjournal = {Front. Med.},
	author = {Jovel, Juan and Greiner, Russell},
	urldate = {2024-06-15},
	date = {2021-12-16},
	note = {Publisher: Frontiers},
	keywords = {Biomedical Research, machine learning, reinforcement learning, supervised learning, unsupervised learning},
}

@article{noauthor_notitle_nodate-1,
}
